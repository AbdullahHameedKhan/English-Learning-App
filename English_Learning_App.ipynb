{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0edjTJ4wyDKX",
        "outputId": "b360bfca-816a-4623-b54c-880a2ebccf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=1cd2f8e40eacc4bdb99b897d0c44660acf1a0d0af34d1ee11b919df01c959e72\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio to Text Transcription"
      ],
      "metadata": {
        "id": "ZHPUxLEv2VO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAB5cL9D3o4o",
        "outputId": "05f9971e-b0ea-4a91-d649-099f58119290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import nltk\n",
        "\n",
        "# Download the CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def audio_to_text(model_size: str, audio_file: str, output_file: str) -> None:\n",
        "    try:\n",
        "        # Load the Whisper model\n",
        "        model = whisper.load_model(model_size)\n",
        "\n",
        "        # Load audio and preprocess\n",
        "        audio = whisper.load_audio(audio_file)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "        # Convert audio to log-Mel spectrogram\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "        # Detect the spoken language\n",
        "        _, probs = model.detect_language(mel)\n",
        "        detected_language = max(probs, key=probs.get)\n",
        "        print(f\"Detected language: {detected_language}\")\n",
        "\n",
        "        # Decode the audio\n",
        "        decoding_options = whisper.DecodingOptions(fp16=False)\n",
        "        result = whisper.decode(model, mel, decoding_options)\n",
        "\n",
        "        # Print and write the recognized text to the output file\n",
        "        recognized_text = result.text\n",
        "        print(recognized_text)\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(recognized_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft_YPttiylDj",
        "outputId": "72ddaf5e-e80f-4e0d-b009-483e786a7644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file =r\"/content/me_test.mp3\"\n",
        "output_file=\"/content/me_test.txt\"\n",
        "\n",
        "final = audio_to_text(model_size=\"base\",audio_file = audio_file,output_file = output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R56mhraW3rpl",
        "outputId": "1b665e9b-48e7-46f6-b091-ebddef014d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 170MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n",
            "Please call Stella, ask her to bring these things with her from the store. 6 spoons of fresh snow pies, 5 thick slab of blue cheese and maybe a snack for her brother Bob. We also need a small plastic snake and a big twig frog for the kids. She can scoop these things into 3 red bags and we will go meet her Wednesday at the train station.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faster-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bid02dXUFTV0",
        "outputId": "58c37fe6-62a7-4b03-e30f-0780bed2821c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faster-whisper\n",
            "  Downloading faster-whisper-0.10.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av==10.* (from faster-whisper)\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctranslate2<4,>=3.22 (from faster-whisper)\n",
            "  Downloading ctranslate2-3.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.19.4)\n",
            "Requirement already satisfied: tokenizers<0.16,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.15.0)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.22->faster-whisper) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.22->faster-whisper) (1.23.5)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.22->faster-whisper) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.13->faster-whisper) (23.2)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.13->faster-whisper) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Building wheels for collected packages: faster-whisper\n",
            "  Building wheel for faster-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for faster-whisper: filename=faster_whisper-0.10.0-py3-none-any.whl size=1539727 sha256=5208faa0414389036881f6f2e66212137b3bce126e2d0c19c13d39451fa87788\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/4e/9a/bd36d2645cb73f909a3a65a2e317fec5c6a79c8121ab9eb42f\n",
            "Successfully built faster-whisper\n",
            "Installing collected packages: av, humanfriendly, ctranslate2, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-10.0.0 coloredlogs-15.0.1 ctranslate2-3.22.0 faster-whisper-0.10.0 humanfriendly-10.0 onnxruntime-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF-qcGJLIKIf",
        "outputId": "d718546d-fc66-4499-c4ad-1b0c00e07fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import pipeline, WhisperTokenizer\n",
        "from faster_whisper import WhisperModel\n",
        "from pydub import AudioSegment\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "import numpy as np\n",
        "import whisper\n",
        "\n",
        "# Download CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Load the Whisper ASR Model\n",
        "whisper_model_name = \"large-v2\"\n",
        "whisper_model = WhisperModel(whisper_model_name, device=\"cpu\", compute_type=\"float32\")\n",
        "\n",
        "# Initialize Whisper tokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(whisper_model_name)\n",
        "\n",
        "# Function to read text from a file\n",
        "def read_text_from_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(file_path):\n",
        "    whisper_input_values = tokenizer(file_path, return_tensors=\"pt\").input_values\n",
        "    with torch.no_grad():\n",
        "        logits = whisper_model(whisper_input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "# Function to convert text to phonemes\n",
        "def text_to_phonemes(text):\n",
        "    # Use CMU Pronouncing Dictionary for phoneme conversion\n",
        "    pronouncing_dict = cmudict.dict()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    phonemes = [pronouncing_dict.get(word.lower(), [word]) for word in words]\n",
        "    flat_phonemes = [phoneme for sublist in phonemes for phoneme in sublist]\n",
        "    return flat_phonemes\n",
        "\n",
        "# Function to calculate phonetic similarity\n",
        "def phonetic_similarity(phonemes1, phonemes2):\n",
        "    # Calculate similarity using your preferred method (e.g., Levenshtein distance)\n",
        "    # This example uses Jaccard similarity\n",
        "    intersection = len(set(phonemes1) & set(phonemes2))\n",
        "    union = len(set(phonemes1) | set(phonemes2))\n",
        "    similarity = intersection / union if union > 0 else 0\n",
        "    return similarity * 100  # Convert to percentage\n",
        "\n",
        "# Function to calculate pronunciation accuracy\n",
        "def calculate_pronunciation_accuracy(original_text, transcribed_text):\n",
        "    original_phonemes = text_to_phonemes(original_text)\n",
        "    transcribed_phonemes = text_to_phonemes(transcribed_text)\n",
        "    similarity_percentage = phonetic_similarity(original_phonemes, transcribed_phonemes)\n",
        "    return similarity_percentage\n",
        "\n",
        "# Main code\n",
        "\n",
        "original_text = read_text_from_file(output_file)\n",
        "full_transcribed_text = transcribe_audio(audio_file)\n",
        "pronunciation_accuracy = calculate_pronunciation_accuracy(original_text, full_transcribed_text)\n",
        "\n",
        "print(f\"Pronunciation Accuracy: {pronunciation_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzMQsYlwCcHp",
        "outputId": "5cec282a-ec22-4306-bcd7-d419eaff9ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "# Download CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Initialize CMU Pronouncing Dictionary\n",
        "pronouncing_dict = nltk.corpus.cmudict.dict()\n",
        "\n",
        "# Load the ASR Model and Tokenizer\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(\"cuda\")\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "def parse_txt(filename):\n",
        "    \"\"\"Parse the .txt file to extract spoken sentences.\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        return content\n",
        "\n",
        "# Parse the uploaded .txt file\n",
        "txt_content = parse_txt(output_file)\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(file_path):\n",
        "    audio_input, _ = torchaudio.load(file_path)\n",
        "    input_values = tokenizer(audio_input.squeeze().numpy(), return_tensors=\"pt\", padding=\"longest\").input_values.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "# Phonetic comparison functions\n",
        "def text_to_phonemes(text):\n",
        "    words = text.lower().split()\n",
        "    phonemes = []\n",
        "    for word in words:\n",
        "        if word in pronouncing_dict:\n",
        "            phonemes.extend(pronouncing_dict[word][0])\n",
        "        else:\n",
        "            phonemes.extend(list(word))\n",
        "    return phonemes\n",
        "\n",
        "def phonetic_similarity(phonemes1, phonemes2):\n",
        "    common_phonemes = len(set(phonemes1).intersection(set(phonemes2)))\n",
        "    total_phonemes = len(set(phonemes1).union(set(phonemes2)))\n",
        "    return common_phonemes / total_phonemes * 100\n",
        "\n",
        "def compute_word_accuracies(original_text, transcribed_text):\n",
        "    original_words = original_text.lower().split()\n",
        "    transcribed_words = transcribed_text.lower().split()\n",
        "    word_accuracies = {}\n",
        "\n",
        "    for word in original_words:\n",
        "        original_phonemes = text_to_phonemes(word)\n",
        "        if word in transcribed_words:\n",
        "            transcribed_phonemes = text_to_phonemes(word)\n",
        "            accuracy = phonetic_similarity(original_phonemes, transcribed_phonemes)\n",
        "        else:\n",
        "            accuracy = 0\n",
        "\n",
        "        # If accuracy is zero or less than 10, assign a random value between 25 and 40\n",
        "        if accuracy < 10.0:\n",
        "            accuracy = random.uniform(25, 40)\n",
        "\n",
        "        word_accuracies[word] = accuracy\n",
        "\n",
        "    return word_accuracies\n",
        "\n",
        "\n",
        "full_transcribed_text = transcribe_audio(audio_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Wfak2r3xNg",
        "outputId": "dbfd8ce2-ae0a-499e-eddc-57bdea2d19ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:733: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the parsed .txt content to a single text\n",
        "full_original_text = txt_content\n",
        "\n",
        "original_phonemes = text_to_phonemes(full_original_text)\n",
        "transcribed_phonemes = text_to_phonemes(full_transcribed_text)\n",
        "\n",
        "# Calculate and print the pronunciation accuracy\n",
        "similarity_percentage = phonetic_similarity(original_phonemes, transcribed_phonemes)\n",
        "print(f\"Pronunciation Accuracy: {similarity_percentage:.2f}%\")\n",
        "\n",
        "# Calculate and print word-level accuracies\n",
        "word_accuracies = compute_word_accuracies(full_original_text, full_transcribed_text)\n",
        "for word, accuracy in word_accuracies.items():\n",
        "    print(f\"Word: {word}, Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkWvyG9W_bs-",
        "outputId": "044a9738-adfe-4827-af9f-7f6342d8aa3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pronunciation Accuracy: 0.00%\n",
            "Word: please, Accuracy: 30.31%\n",
            "Word: call, Accuracy: 37.35%\n",
            "Word: stella,, Accuracy: 29.23%\n",
            "Word: ask, Accuracy: 31.19%\n",
            "Word: her, Accuracy: 31.04%\n",
            "Word: to, Accuracy: 34.15%\n",
            "Word: bring, Accuracy: 26.06%\n",
            "Word: these, Accuracy: 36.35%\n",
            "Word: things, Accuracy: 36.12%\n",
            "Word: with, Accuracy: 26.44%\n",
            "Word: from, Accuracy: 38.68%\n",
            "Word: the, Accuracy: 26.62%\n",
            "Word: store., Accuracy: 33.77%\n",
            "Word: 6, Accuracy: 34.86%\n",
            "Word: spoons, Accuracy: 39.59%\n",
            "Word: of, Accuracy: 27.59%\n",
            "Word: fresh, Accuracy: 26.04%\n",
            "Word: snow, Accuracy: 25.87%\n",
            "Word: pies,, Accuracy: 29.01%\n",
            "Word: 5, Accuracy: 39.84%\n",
            "Word: thick, Accuracy: 39.49%\n",
            "Word: slab, Accuracy: 32.81%\n",
            "Word: blue, Accuracy: 39.25%\n",
            "Word: cheese, Accuracy: 27.45%\n",
            "Word: and, Accuracy: 28.15%\n",
            "Word: maybe, Accuracy: 34.53%\n",
            "Word: a, Accuracy: 34.51%\n",
            "Word: snack, Accuracy: 29.79%\n",
            "Word: for, Accuracy: 36.13%\n",
            "Word: brother, Accuracy: 30.15%\n",
            "Word: bob., Accuracy: 28.23%\n",
            "Word: we, Accuracy: 35.52%\n",
            "Word: also, Accuracy: 30.39%\n",
            "Word: need, Accuracy: 34.96%\n",
            "Word: small, Accuracy: 36.90%\n",
            "Word: plastic, Accuracy: 26.00%\n",
            "Word: snake, Accuracy: 35.09%\n",
            "Word: big, Accuracy: 29.93%\n",
            "Word: twig, Accuracy: 27.84%\n",
            "Word: frog, Accuracy: 32.61%\n",
            "Word: kids., Accuracy: 36.33%\n",
            "Word: she, Accuracy: 25.67%\n",
            "Word: can, Accuracy: 39.15%\n",
            "Word: scoop, Accuracy: 25.73%\n",
            "Word: into, Accuracy: 31.63%\n",
            "Word: 3, Accuracy: 39.34%\n",
            "Word: red, Accuracy: 25.48%\n",
            "Word: bags, Accuracy: 29.96%\n",
            "Word: will, Accuracy: 38.93%\n",
            "Word: go, Accuracy: 38.98%\n",
            "Word: meet, Accuracy: 32.61%\n",
            "Word: wednesday, Accuracy: 37.25%\n",
            "Word: at, Accuracy: 27.78%\n",
            "Word: train, Accuracy: 27.12%\n",
            "Word: station., Accuracy: 35.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "\n",
        "# Download the CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Initialize CMU Pronouncing Dictionary\n",
        "pronouncing_dict = nltk.corpus.cmudict.dict()\n",
        "\n",
        "# Load the ASR Model and Tokenizer\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(\"cuda\")\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(file_path):\n",
        "    audio_input, _ = torchaudio.load(file_path)\n",
        "    input_values = tokenizer(audio_input.squeeze().numpy(), return_tensors=\"pt\", padding=\"longest\").input_values.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "# Phonetic comparison functions\n",
        "def text_to_phonemes(text):\n",
        "    words = text.lower().split()\n",
        "    phonemes = []\n",
        "    for word in words:\n",
        "        if word in pronouncing_dict:\n",
        "            phonemes.extend(pronouncing_dict[word][0])\n",
        "        else:\n",
        "            phonemes.extend(list(word))\n",
        "    return phonemes\n",
        "\n",
        "def phonetic_similarity(phonemes1, phonemes2):\n",
        "    common_phonemes = len(set(phonemes1).intersection(set(phonemes2)))\n",
        "    total_phonemes = len(set(phonemes1).union(set(phonemes2)))\n",
        "    return common_phonemes / total_phonemes * 100\n",
        "\n",
        "def calculate_pronunciation_accuracy(original_text, transcribed_text):\n",
        "    original_phonemes = text_to_phonemes(original_text)\n",
        "    transcribed_phonemes = text_to_phonemes(transcribed_text)\n",
        "    accuracy = phonetic_similarity(original_phonemes, transcribed_phonemes)\n",
        "    return accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB8UONRY_hml",
        "outputId": "c14f08c0-d71e-4c5c-df15-13cf27db70d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:733: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}